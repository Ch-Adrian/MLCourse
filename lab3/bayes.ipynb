{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.datasets import fetch_openml, load_iris, load_breast_cancer, fetch_20newsgroups\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.base import BaseEstimator\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "X = iris_dataset.data\n",
        "y = iris_dataset.target"
      ],
      "outputs": [],
      "execution_count": 220,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Gaussowski Naiwny Klasyfikator Bayesa"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie 1"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Bayes(BaseEstimator):\n",
        "    def __init__(self, *, param=1):\n",
        "        self.param = param\n",
        "        self.C = []\n",
        "        self.amt_of_C = []\n",
        "        self.Cpriors = []\n",
        "        self.mean = []\n",
        "        self.std = []\n",
        "\n",
        "    def fit(self, X_train, y_train=None):\n",
        "        self.C = np.unique(y_train)\n",
        "        for c in self.C:\n",
        "            self.amt_of_C.append(np.sum(y_train == c))\n",
        "\n",
        "        self.Cpriors = [c / len(y_train) for c in self.amt_of_C]\n",
        "\n",
        "        self.mean = []\n",
        "        self.std = []\n",
        "\n",
        "        for c in self.C:\n",
        "            X = X_train[y_train == c]\n",
        "            self.mean.append(np.mean(X, axis=0))\n",
        "            self.std.append(np.std(X, axis=0))\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        post_arg = []\n",
        "        \n",
        "        for row in X:\n",
        "\n",
        "            max_posteriori_idx = -1\n",
        "            max_posteriori = -np.inf\n",
        "            for c_idx, c in enumerate(self.C):\n",
        "\n",
        "                posteriori = 0\n",
        "                for x_idx, x in enumerate(row):\n",
        "                    mean = self.mean[c_idx][x_idx]\n",
        "                    std = self.std[c_idx][x_idx]\n",
        "                    \n",
        "                    posteriori += np.log(self.likelihood(x, mean, std))\n",
        "                posteriori += np.log(self.Cpriors[c_idx])\n",
        "\n",
        "                if posteriori > max_posteriori:\n",
        "                    max_posteriori = posteriori\n",
        "                    max_posteriori_idx = c_idx\n",
        "            \n",
        "            post_arg.append(max_posteriori_idx)\n",
        "        \n",
        "        return post_arg\n",
        "\n",
        "    \n",
        "    def likelihood(self, x, mean, std):\n",
        "        return np.exp(-((x - mean) ** 2 / (2 * std ** 2))) / (np.sqrt(2 * np.pi * std))"
      ],
      "outputs": [],
      "execution_count": 221,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie 3"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "Bayesian_scores = {\n",
        "    'accuracy': [],\n",
        "    'f1': [],\n",
        "    'precision': []\n",
        "}\n",
        "\n",
        "GaussianNB_scores = {\n",
        "    'accuracy': [],\n",
        "    'f1': [],\n",
        "    'precision': []\n",
        "}\n",
        "\n",
        "for i in range(20):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=i+np.random.randint(0, 100))\n",
        "    clf_Bayes = Bayes()\n",
        "    clf_Bayes.fit(X_train, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test)\n",
        "    Bayesian_scores[\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    Bayesian_scores[\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    Bayesian_scores[\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    clf_GaussianNB = GaussianNB()\n",
        "    clf_GaussianNB.fit(X_train, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test)\n",
        "    GaussianNB_scores[\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    GaussianNB_scores[\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    GaussianNB_scores[\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "print(\"Bayesian\")\n",
        "print(\"Accuracy: \", np.mean(Bayesian_scores[\"accuracy\"]))\n",
        "print(\"F1: \", np.mean(Bayesian_scores[\"f1\"]))\n",
        "print(\"Precision: \", np.mean(Bayesian_scores[\"precision\"]))\n",
        "print(\"\")\n",
        "print(\"GaussianNB\")\n",
        "print(\"Accuracy: \", np.mean(GaussianNB_scores[\"accuracy\"]))\n",
        "print(\"F1: \", np.mean(GaussianNB_scores[\"f1\"]))\n",
        "print(\"Precision: \", np.mean(GaussianNB_scores[\"precision\"]))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Bayesian\nAccuracy:  0.9549999999999998\nF1:  0.9551457882329567\nPrecision:  0.9601374236879531\n\nGaussianNB\nAccuracy:  0.9574999999999998\nF1:  0.9575988722597664\nPrecision:  0.9621128185866172\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n"
        }
      ],
      "execution_count": 222,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie 4"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_data = load_breast_cancer()\n",
        "X = breast_cancer_data.data\n",
        "y = breast_cancer_data.target\n",
        "X = np.where(X == 0, 1e-6, X)"
      ],
      "outputs": [],
      "execution_count": 223,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'cosine']\n",
        "\n",
        "for kernel_name in kernels:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i+np.random.randint(0, 100))\n",
        "    kernel_pca = KernelPCA(n_components=2, kernel=kernel_name)\n",
        "    clf_Bayes = Bayes()\n",
        "    clf_GaussianNB = GaussianNB()\n",
        "    clf_RandomForest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
        "\n",
        "    X_train_pca = kernel_pca.fit_transform(X_train)\n",
        "    X_test_pca = kernel_pca.transform(X_test)\n",
        "\n",
        "    print(\"========== Kernel: \", kernel_name, \" ==========\")\n",
        "    # KernelPCA Bayes\n",
        "    print(\"Bayes:\")\n",
        "    clf_Bayes.fit(X_train_pca, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test_pca)\n",
        "    print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
        "    print(\"F1: \", metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Precision: \", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # KernelPCA GaussianNB\n",
        "    print(\"GaussianNB:\")\n",
        "    clf_GaussianNB.fit(X_train_pca, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test_pca)\n",
        "    print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
        "    print(\"F1: \", metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Precision: \", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # KernelPCA RandomForest\n",
        "    print(\"RandomForest:\")\n",
        "    clf_RandomForest.fit(X_train_pca, y_train)\n",
        "    y_pred = clf_RandomForest.predict(X_test_pca)\n",
        "    print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
        "    print(\"F1: \", metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"Precision: \", metrics.precision_score(y_test, y_pred, average='weighted'))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "========== Kernel:  linear  ==========\nBayes:\nAccuracy:  0.9181286549707602\nF1:  0.9187853107344632\nPrecision:  0.9205166418281173\nGaussianNB:\nAccuracy:  0.9122807017543859\nF1:  0.9112099854313502\nPrecision:  0.9119263089851325\nRandomForest:\nAccuracy:  0.9181286549707602\nF1:  0.9181286549707602\nPrecision:  0.9181286549707602\n========== Kernel:  poly  ==========\nBayes:\nAccuracy:  0.935672514619883\nF1:  0.9347065550615989\nPrecision:  0.937305276486563\nGaussianNB:\nAccuracy:  0.9122807017543859\nF1:  0.9094968393144929\nPrecision:  0.9193499500784482\nRandomForest:\nAccuracy:  0.935672514619883\nF1:  0.935294360053832\nPrecision:  0.935549381030646\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "========== Kernel:  rbf  ==========\nBayes:\nAccuracy:  0.6666666666666666\nF1:  0.5333333333333333\nPrecision:  0.4444444444444444\nGaussianNB:\nAccuracy:  0.6666666666666666\nF1:  0.5333333333333333\nPrecision:  0.4444444444444444\nRandomForest:\nAccuracy:  0.6666666666666666\nF1:  0.5333333333333333\nPrecision:  0.4444444444444444\n========== Kernel:  sigmoid  ==========\nBayes:\nAccuracy:  0.0\nF1:  0.0\nPrecision:  0.0\nGaussianNB:\nAccuracy:  0.3567251461988304\nF1:  0.18758822343214357\nPrecision:  0.12725282993057693\nRandomForest:\nAccuracy:  0.6432748538011696\nF1:  0.503631558136147\nPrecision:  0.41380253753291607\n========== Kernel:  cosine  ==========\nBayes:\nAccuracy:  0.8830409356725146\nF1:  0.8830409356725146\nPrecision:  0.8830409356725146\nGaussianNB:\nAccuracy:  0.8947368421052632\nF1:  0.893730407523511\nPrecision:  0.8938279501119285\nRandomForest:\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/tmp/ipykernel_5223/3562722749.py:54: RuntimeWarning: invalid value encountered in scalar divide\n  return np.exp(-((x - mean) ** 2 / (2 * std ** 2))) / (np.sqrt(2 * np.pi * std))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/naive_bayes.py:510: RuntimeWarning: divide by zero encountered in log\n  n_ij = -0.5 * np.sum(np.log(2.0 * np.pi * self.var_[i, :]))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/naive_bayes.py:511: RuntimeWarning: invalid value encountered in divide\n  n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) / (self.var_[i, :]), 1)\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Accuracy:  0.9064327485380117\nF1:  0.9055381400208987\nPrecision:  0.9057669758021533\n"
        }
      ],
      "execution_count": 224,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel 'poly' daje najlepsze rezultaty."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "choosen_kernel = 'poly'"
      ],
      "outputs": [],
      "execution_count": 225,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "Bayesian_scores = {\n",
        "    'basic': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'scaler': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'pca': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'kernel_pca': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'box_cox': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    }\n",
        "}\n",
        "\n",
        "GaussianNB_scores = {\n",
        "    'basic': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'scaler': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'pca': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'kernel_pca': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'box_cox': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    }\n",
        "}\n",
        "\n",
        "RandomForest_scores = {\n",
        "    'basic': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'scaler': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'pca': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'kernel_pca': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    },\n",
        "    'box_cox': {\n",
        "        'accuracy': [],\n",
        "        'f1': [],\n",
        "        'precision': []\n",
        "    }\n",
        "}\n",
        "\n",
        "for i in range(20):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i+np.random.randint(0, 100))\n",
        "    \n",
        "    clf_Bayes = Bayes()\n",
        "    scaler = StandardScaler()\n",
        "    pca = PCA(n_components=2)\n",
        "    kernel_pca = KernelPCA(n_components=2, kernel=choosen_kernel)\n",
        "    boxcox = PowerTransformer(method=\"box-cox\", standardize=True)\n",
        "    clf_GaussianNB = GaussianNB()\n",
        "    clf_RandomForest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
        "    \n",
        "    X_train_scaler = scaler.fit_transform(X_train)\n",
        "    X_test_scaler = scaler.transform(X_test)\n",
        "\n",
        "    X_train_kernel_pca = kernel_pca.fit_transform(X_train)\n",
        "    X_test_kernel_pca = kernel_pca.transform(X_test)\n",
        "    \n",
        "    X_train_box_cox = boxcox.fit_transform(X_train)\n",
        "    X_test_box_cox = boxcox.transform(X_test)\n",
        "\n",
        "    # Basic Bayes\n",
        "    clf_Bayes.fit(X_train, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test)\n",
        "    Bayesian_scores[\"basic\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    Bayesian_scores[\"basic\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    Bayesian_scores[\"basic\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Scaler Bayes\n",
        "    clf_Bayes.fit(X_train_scaler, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test_scaler)\n",
        "    Bayesian_scores[\"scaler\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    Bayesian_scores[\"scaler\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    Bayesian_scores[\"scaler\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # PCA Bayes\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    clf_Bayes.fit(X_train_pca, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test_pca)\n",
        "    Bayesian_scores[\"pca\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    Bayesian_scores[\"pca\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    Bayesian_scores[\"pca\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # KernelPCA Bayes\n",
        "    clf_Bayes.fit(X_train_kernel_pca, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test_kernel_pca)\n",
        "    Bayesian_scores[\"kernel_pca\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    Bayesian_scores[\"kernel_pca\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    Bayesian_scores[\"kernel_pca\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Box-Cox Bayes\n",
        "    clf_Bayes.fit(X_train_box_cox, y_train)\n",
        "    y_pred = clf_Bayes.predict(X_test_box_cox)\n",
        "    Bayesian_scores[\"box_cox\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    Bayesian_scores[\"box_cox\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    Bayesian_scores[\"box_cox\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Basic GaussianNB\n",
        "    clf_GaussianNB.fit(X_train, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test)\n",
        "    GaussianNB_scores[\"basic\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    GaussianNB_scores[\"basic\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    GaussianNB_scores[\"basic\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Scaler GaussianNB\n",
        "    clf_GaussianNB.fit(X_train_scaler, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test_scaler)\n",
        "    GaussianNB_scores[\"scaler\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    GaussianNB_scores[\"scaler\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    GaussianNB_scores[\"scaler\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # PCA GaussianNB\n",
        "    clf_GaussianNB.fit(X_train_pca, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test_pca)\n",
        "    GaussianNB_scores[\"pca\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    GaussianNB_scores[\"pca\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    GaussianNB_scores[\"pca\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # KernelPCA GaussianNB\n",
        "    clf_GaussianNB.fit(X_train_kernel_pca, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test_kernel_pca)\n",
        "    GaussianNB_scores[\"kernel_pca\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    GaussianNB_scores[\"kernel_pca\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    GaussianNB_scores[\"kernel_pca\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Box-Cox GaussianNB\n",
        "    clf_GaussianNB.fit(X_train_box_cox, y_train)\n",
        "    y_pred = clf_GaussianNB.predict(X_test_box_cox)\n",
        "    GaussianNB_scores[\"box_cox\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    GaussianNB_scores[\"box_cox\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    GaussianNB_scores[\"box_cox\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Basic RandomForest\n",
        "    clf_RandomForest.fit(X_train, y_train)\n",
        "    y_pred = clf_RandomForest.predict(X_test)\n",
        "    RandomForest_scores[\"basic\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    RandomForest_scores[\"basic\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    RandomForest_scores[\"basic\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Scaler RandomForest\n",
        "    clf_RandomForest.fit(X_train_scaler, y_train)\n",
        "    y_pred = clf_RandomForest.predict(X_test_scaler)\n",
        "    RandomForest_scores[\"scaler\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    RandomForest_scores[\"scaler\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    RandomForest_scores[\"scaler\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # PCA RandomForest\n",
        "    clf_RandomForest.fit(X_train_pca, y_train)\n",
        "    y_pred = clf_RandomForest.predict(X_test_pca)\n",
        "    RandomForest_scores[\"pca\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    RandomForest_scores[\"pca\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    RandomForest_scores[\"pca\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # KernelPCA RandomForest\n",
        "    clf_RandomForest.fit(X_train_kernel_pca, y_train)\n",
        "    y_pred = clf_RandomForest.predict(X_test_kernel_pca)\n",
        "    RandomForest_scores[\"kernel_pca\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    RandomForest_scores[\"kernel_pca\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    RandomForest_scores[\"kernel_pca\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "    # Box-Cox RandomForest\n",
        "    clf_RandomForest.fit(X_train_box_cox, y_train)\n",
        "    y_pred = clf_RandomForest.predict(X_test_box_cox)\n",
        "    RandomForest_scores[\"box_cox\"][\"accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
        "    RandomForest_scores[\"box_cox\"][\"f1\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    RandomForest_scores[\"box_cox\"][\"precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "\n",
        "\n",
        "for (Bayesian, GNB, RandomForest) in zip(Bayesian_scores.items(), GaussianNB_scores.items(), RandomForest_scores.items()):\n",
        "    print(Bayesian[0])\n",
        "    for key in \"accuracy f1 precision\".split():\n",
        "        print(\"\\t====== \", key, \" ======\")\n",
        "        print(\"\\tBayesian: \", np.mean(Bayesian[1][key]))\n",
        "        print(\"\\tGaussianNB: \", np.mean(GNB[1][key]))\n",
        "        print(\"\\tRandomForest: \", np.mean(RandomForest[1][key]))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n/tmp/ipykernel_5223/3562722749.py:41: RuntimeWarning: divide by zero encountered in log\n  posteriori += np.log(self.likelihood(x, mean, std))\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "basic\n\t======  accuracy  ======\n\tBayesian:  0.9362573099415205\n\tGaussianNB:  0.9394736842105266\n\tRandomForest:  0.941812865497076\n\t======  f1  ======\n\tBayesian:  0.936788619034685\n\tGaussianNB:  0.9391534944678996\n\tRandomForest:  0.941536669405469\n\t======  precision  ======\n\tBayesian:  0.940089005257612\n\tGaussianNB:  0.9404064997230502\n\tRandomForest:  0.9431895905566205\nscaler\n\t======  accuracy  ======\n\tBayesian:  0.9362573099415205\n\tGaussianNB:  0.9350877192982457\n\tRandomForest:  0.941812865497076\n\t======  f1  ======\n\tBayesian:  0.936788619034685\n\tGaussianNB:  0.9349647937459571\n\tRandomForest:  0.941536669405469\n\t======  precision  ======\n\tBayesian:  0.940089005257612\n\tGaussianNB:  0.936066592511063\n\tRandomForest:  0.9431895905566205\npca\n\t======  accuracy  ======\n\tBayesian:  0.8973684210526317\n\tGaussianNB:  0.904093567251462\n\tRandomForest:  0.9192982456140351\n\t======  f1  ======\n\tBayesian:  0.8973128920232819\n\tGaussianNB:  0.9017915475724674\n\tRandomForest:  0.9179392074062456\n\t======  precision  ======\n\tBayesian:  0.8989062020681112\n\tGaussianNB:  0.9085606190915962\n\tRandomForest:  0.9215769327091916\nkernel_pca\n\t======  accuracy  ======\n\tBayesian:  0.9011695906432748\n\tGaussianNB:  0.888888888888889\n\tRandomForest:  0.908187134502924\n\t======  f1  ======\n\tBayesian:  0.8993524033777403\n\tGaussianNB:  0.8848682711718852\n\tRandomForest:  0.9063709507914517\n\t======  precision  ======\n\tBayesian:  0.9048179150689972\n\tGaussianNB:  0.8966324567951689\n\tRandomForest:  0.9112704147089536\nbox_cox\n\t======  accuracy  ======\n\tBayesian:  0.9508771929824558\n\tGaussianNB:  0.9479532163742688\n\tRandomForest:  0.941812865497076\n\t======  f1  ======\n\tBayesian:  0.9507422216413117\n\tGaussianNB:  0.9479041085419487\n\tRandomForest:  0.941536669405469\n\t======  precision  ======\n\tBayesian:  0.951930794562065\n\tGaussianNB:  0.9489968359559111\n\tRandomForest:  0.9431895905566205\n"
        }
      ],
      "execution_count": 226,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Naiwny Klasyfikator Bayesa z rozkładem Bernoulliego"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesBernoulli(BaseEstimator):\n",
        "    def __init__(self, *, param=1):\n",
        "        self.param = param\n",
        "        self.C = []\n",
        "        self.amt_of_C = []\n",
        "        self.Cpriors = []\n",
        "        self.pxC = []\n",
        "\n",
        "    def fit(self, X_train, y_train=None):\n",
        "        self.C = np.unique(y_train)\n",
        "        for c in self.C:\n",
        "            self.amt_of_C.append(np.sum(y_train == c))\n",
        "\n",
        "        self.Cpriors = [ (c / len(y_train)).astype(np.float64) for c in self.amt_of_C]\n",
        "        self.pxC = [ [] for _ in range(len(self.C)) ]\n",
        "\n",
        "        for idx_c, c in enumerate(self.C):\n",
        "            XC = X_train[y_train == c].astype(np.float64)\n",
        "            self.pxC[idx_c] = (XC.sum(axis=0).astype(np.float64) / XC.shape[0])\n",
        "            self.pxC[idx_c] = np.squeeze(self.pxC[idx_c])\n",
        "            self.pxC[idx_c] = np.where(self.pxC[idx_c] == 0, 1, self.pxC[idx_c]).astype(np.float64)\n",
        "            self.pxC[idx_c] = np.squeeze(self.pxC[idx_c])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        post_arg = []\n",
        "        \n",
        "        for row in X:\n",
        "            max_posteriori_idx = -1\n",
        "            max_posteriori = -np.inf\n",
        "            for c_idx, c in enumerate(self.C):\n",
        "\n",
        "                posteriori = 1\n",
        "                for x_idx, x in enumerate(row):\n",
        "                    posteriori *= self.pxC[c_idx][x_idx]\n",
        "                \n",
        "                posteriori *= self.Cpriors[c_idx]\n",
        "                posteriori = posteriori.astype(np.float64)\n",
        "                \n",
        "                if posteriori > max_posteriori:\n",
        "                    max_posteriori = posteriori\n",
        "                    max_posteriori_idx = c_idx\n",
        "            \n",
        "            post_arg.append(self.C[max_posteriori_idx])\n",
        "        \n",
        "        return post_arg\n"
      ],
      "outputs": [],
      "execution_count": 227,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "X_train, X_test, y_train, y_test = newsgroups_train.data, newsgroups_test.data, newsgroups_train.target, newsgroups_test.target\n",
        "X_train = cv.fit_transform(X_train)\n",
        "X_test = cv.transform(X_test)"
      ],
      "outputs": [],
      "execution_count": 228,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "BBnb = BayesBernoulli()\n",
        "BBnb.fit(X_train, y_train)\n",
        "y_pred_bbnb = BBnb.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_bbnb))\n",
        "print(sum(y_pred_bbnb == y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       319\n           1       0.00      0.00      0.00       389\n           2       0.00      0.00      0.00       394\n           3       0.00      0.00      0.00       392\n           4       0.00      0.00      0.00       385\n           5       0.00      0.00      0.00       395\n           6       0.05      1.00      0.10       390\n           7       0.00      0.00      0.00       396\n           8       0.00      0.00      0.00       398\n           9       0.00      0.00      0.00       397\n          10       0.00      0.00      0.00       399\n          11       0.00      0.00      0.00       396\n          12       0.00      0.00      0.00       393\n          13       0.00      0.00      0.00       396\n          14       0.00      0.00      0.00       394\n          15       0.00      0.00      0.00       398\n          16       0.00      0.00      0.00       364\n          17       0.00      0.00      0.00       376\n          18       0.00      0.00      0.00       310\n          19       0.00      0.00      0.00       251\n\n    accuracy                           0.05      7532\n   macro avg       0.00      0.05      0.00      7532\nweighted avg       0.00      0.05      0.01      7532\n\n390\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        }
      ],
      "execution_count": 229,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB(binarize=0.0)\n",
        "model = bnb.fit(X_train, y_train)\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(sum(y_pred == y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n           0       0.92      0.32      0.47       319\n           1       0.58      0.63      0.61       389\n           2       0.33      0.01      0.01       394\n           3       0.43      0.81      0.56       392\n           4       0.64      0.76      0.70       385\n           5       0.84      0.61      0.70       395\n           6       0.30      0.93      0.45       390\n           7       0.67      0.78      0.72       396\n           8       0.74      0.91      0.82       398\n           9       0.77      0.87      0.82       397\n          10       0.99      0.83      0.90       399\n          11       0.82      0.69      0.75       396\n          12       0.57      0.67      0.62       393\n          13       0.84      0.52      0.64       396\n          14       0.88      0.68      0.77       394\n          15       0.53      0.80      0.64       398\n          16       0.74      0.57      0.64       364\n          17       0.96      0.65      0.78       376\n          18       0.89      0.19      0.31       310\n          19       1.00      0.00      0.01       251\n\n    accuracy                           0.63      7532\n   macro avg       0.72      0.61      0.60      7532\nweighted avg       0.71      0.63      0.61      7532\n\n4751\n"
        }
      ],
      "execution_count": 230,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Naiwny Klasyfikator Bayesa dla zbioru danych Adult Income"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adult = fetch_openml(\"adult\", version=2)  \n",
        "X = adult.data\n",
        "y = adult.target"
      ],
      "outputs": [],
      "execution_count": 231,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_adult_X = pd.DataFrame(X, columns=adult.feature_names)\n",
        "print(df_adult_X.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   age  workclass  fnlwgt     education  education-num      marital-status  \\\n0   25    Private  226802          11th              7       Never-married   \n1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n3   44    Private  160323  Some-college             10  Married-civ-spouse   \n4   18        NaN  103497  Some-college             10       Never-married   \n\n          occupation relationship   race     sex  capital-gain  capital-loss  \\\n0  Machine-op-inspct    Own-child  Black    Male             0             0   \n1    Farming-fishing      Husband  White    Male             0             0   \n2    Protective-serv      Husband  White    Male             0             0   \n3  Machine-op-inspct      Husband  Black    Male          7688             0   \n4                NaN    Own-child  White  Female             0             0   \n\n   hours-per-week native-country  \n0              40  United-States  \n1              50  United-States  \n2              40  United-States  \n3              40  United-States  \n4              30  United-States  \n"
        }
      ],
      "execution_count": 232,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult_y = pd.DataFrame(y)\n",
        "df_adult_y.rename(columns={'class': 'income'}, inplace=True)\n",
        "print(df_adult_y.income.value_counts())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "income\n<=50K    37155\n>50K     11687\nName: count, dtype: int64\n"
        }
      ],
      "execution_count": 233,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "le = sklearn.preprocessing.LabelEncoder()\n",
        "le.fit(df_adult_y)\n",
        "df_adult_y = le.transform(df_adult_y)\n",
        "print(df_adult_y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[0 0 1 ... 0 0 1]\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/adrian/anaconda3/envs/um-labs/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
        }
      ],
      "execution_count": 234,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \n",
        "                 \"capital-gain\", \"capital-loss\",\"hours-per-week\", \"native-country\"]\n",
        "\n",
        "categorical_features_names = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", 'race', 'sex', 'native-country']\n",
        "categorical_features = [1, 3, 5, 6, 7, 8, 9, 13]"
      ],
      "outputs": [],
      "execution_count": 235,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feature_classes = {}\n",
        "for feature in categorical_features_names:\n",
        "    le = sklearn.preprocessing.LabelEncoder()\n",
        "    le.fit(df_adult_X[feature])\n",
        "    df_adult_X[feature] = le.transform(df_adult_X[feature])\n",
        "    feature_classes[feature] = le.classes_"
      ],
      "outputs": [],
      "execution_count": 236,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_adult_X.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   age  workclass  fnlwgt  education  education-num  marital-status  \\\n0   25          3  226802          1              7               4   \n1   38          3   89814         11              9               2   \n2   28          1  336951          7             12               2   \n3   44          3  160323         15             10               2   \n4   18          8  103497         15             10               4   \n\n   occupation  relationship  race  sex  capital-gain  capital-loss  \\\n0           6             3     2    1             0             0   \n1           4             0     4    1             0             0   \n2          10             0     4    1             0             0   \n3           6             0     2    1          7688             0   \n4          14             3     4    0             0             0   \n\n   hours-per-week  native-country  \n0              40              38  \n1              50              38  \n2              40              38  \n3              40              38  \n4              30              38  \n"
        }
      ],
      "execution_count": 237,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult_X2 = df_adult_X.copy()\n",
        "\n",
        "for col in categorical_features_names:\n",
        "    df_adult_X2[col] = df_adult_X2[col].astype(str).fillna('missing')\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 238,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for col in feature_names:\n",
        "    if col not in categorical_features_names:\n",
        "        df_adult_X2[col] = df_adult_X2[col].astype(float).fillna(0)"
      ],
      "outputs": [],
      "execution_count": 239,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_adult_X2, df_adult_y, test_size=0.3, random_state=np.random.randint(0, 100))"
      ],
      "outputs": [],
      "execution_count": 240,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "clf_GaussianNB = GaussianNB()\n",
        "clf_GaussianNB.fit(X_train, y_train)\n",
        "y_pred = clf_GaussianNB.predict(X_test)\n",
        "print(\"GaussianNB\")\n",
        "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"F1: \", metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Precision: \", metrics.precision_score(y_test, y_pred, average='weighted'))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "GaussianNB\nAccuracy:  0.7957414863850406\nF1:  0.7655591197807137\nPrecision:  0.7761256922788576\n"
        }
      ],
      "execution_count": 241,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "um-labs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}